# -*- coding: utf-8 -*-
"""StackOverFlow_Selection_algo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VnVaojbIPVuMq4cRT6hYZ3lCv9pAuehT

# Import Libraries
"""

import pandas as pd
import re
import numpy as np
import seaborn as sns
from pathlib import Path
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc, silhouette_score, davies_bouldin_score, silhouette_samples

from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from imblearn.over_sampling import SMOTE

from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
import pickle

"""# Data Loading"""

# Convert CSV to Excel (If required)
# 1. Define the path
folder_path = Path("../../Profile_Data")
file_to_convert = "StackOverflow-20K-Formatted.csv"
csv_path = folder_path / file_to_convert

if csv_path.exists():
    # 2. READ: The key is 'sep=;' which matches the file format
    df = pd.read_csv(csv_path, sep=';')

    # 3. CONVERT: Change extension to .xlsx
    xlsx_path = csv_path.with_suffix('.xlsx')

    # 4. SAVE: index=False
    df.to_excel(xlsx_path, index=False)

    # 5. Load StackOverflow data
    df = pd.read_excel("../../Profile_Data/StackOverflow-20K-Formatted.xlsx")
    print(f"Successfully converted to Excel!")
    print(f"Dataset: {df.shape}") # (20000, 7)
    print(f"\nColumns: {list(df.columns)}")
    print(df.head()) # Shows the first 5 rows in the formatted table
else:
    print(f"Error: Could not find {csv_path}")

"""# Data Cleaning"""

def clean_data(df):
    print(f"Original shape: {df.shape}")

    # Remove duplicates
    df = df.drop_duplicates()
    print(f"After removing duplicates: {df.shape}")

    # Clean column names
    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')

    # Clean text columns (remove special characters)
    def clean_text(text):
        if pd.isna(text):
            return text
        return re.sub(r'[^a-zA-Z0-9\s]', '', str(text))

    # Apply to text columns that typically have special characters
    text_columns = ['name', 'location', 'username', 'bio', 'company']
    for col in text_columns:
        if col in df.columns:
            df[col] = df[col].apply(clean_text)
            print(f"   Cleaned special chars from: {col}")

    # Fill missing values
    for col in df.columns:
        if df[col].dtype in ['float64', 'int64']:
            df[col] = df[col].fillna(0)
        else:
            df[col] = df[col].fillna('Unknown')

    # Remove completely empty columns
    df = df.dropna(axis=1, how='all')

    # Remove completely empty rows
    df = df.dropna(axis=0, how='all')

    print(f"Final cleaned shape: {df.shape}")

    return df


# Clean the data
stack = clean_data(df)
df

"""## Features"""

all_features = ['Reputation', 'Gold_Badges', 'Silver_Badges', 'Bronze_Badges']
X = df[all_features]

X_with_target = X.copy()

correlation_matrix = X_with_target.corr()

plt.figure(figsize=(14, 12))
sns.heatmap(
    correlation_matrix,
    annot=True,
    fmt='.2f',
    cmap=sns.cubehelix_palette(start=.5, rot=-.75, as_cmap=True),
    center=0,
    square=True,
    linewidths=0.5,
    cbar_kws={'label': 'Correlation Coefficient'}
)
plt.title('StackOverflow Feature Correlation Heatmap', fontsize=16, fontweight='bold', pad=20)
plt.xlabel('Features', fontsize=12)
plt.ylabel('Features', fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.savefig('../ML_Visualizations/stackoverflow/stackoverflow_correlation_heatmap.png', dpi=300, bbox_inches='tight')
plt.show()

"""# Scaling"""

# Scale the data (important for K-means)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
print(X_scaled)

"""# K-Means"""

# Elbow Method to find optimal clusters
print("Testing different numbers of clusters...")
inertias = []
silhouette_scores = []
davies_bouldin_scores = []
K_range = range(2, 11)

for k in K_range:
    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans_temp.fit(X_scaled)
    inertias.append(kmeans_temp.inertia_)
    silhouette_scores.append(silhouette_score(X_scaled, kmeans_temp.labels_))
    davies_bouldin_scores.append(davies_bouldin_score(X_scaled, kmeans_temp.labels_))

# Plot Elbow + Silhouette + Davies-Bouldin
fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))

# Elbow Plot
ax1.plot(K_range, inertias, 'bx-', linewidth=2, markersize=10)
ax1.set_xlabel('Number of Clusters (k)', fontsize=12)
ax1.set_ylabel('Inertia (Within-Cluster Sum of Squares)', fontsize=12)
ax1.set_title('Elbow Method For Optimal k', fontsize=14, fontweight='bold')
ax1.grid(True, alpha=0.3)

# Silhouette Score Plot
ax2.plot(K_range, silhouette_scores, 'rx-', linewidth=2, markersize=10)
ax2.set_xlabel('Number of Clusters (k)', fontsize=12)
ax2.set_ylabel('Silhouette Score', fontsize=12)
ax2.set_title('Silhouette Score vs Number of Clusters', fontsize=14, fontweight='bold')
ax2.axhline(y=0.5, color='green', linestyle='--', label='Good Threshold (0.5)')
ax2.legend()
ax2.grid(True, alpha=0.3)

# Davies-Bouldin Index Plot
ax3.plot(K_range, davies_bouldin_scores, 'gx-', linewidth=2, markersize=10)
ax3.set_xlabel('Number of Clusters (k)', fontsize=12)
ax3.set_ylabel('Davies-Bouldin Index', fontsize=12)
ax3.set_title('Davies-Bouldin Index vs Number of Clusters', fontsize=14, fontweight='bold')
ax3.axhline(y=1.0, color='red', linestyle='--', label='Good Threshold (< 1.0)')
ax3.legend()
ax3.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Print optimal k based on silhouette score
optimal_k = K_range[np.argmax(silhouette_scores)]
print(f"\n Optimal number of clusters based on Silhouette Score: {optimal_k}")
print(f"   Silhouette Score: {max(silhouette_scores):.3f}")
print(f"   Davies-Bouldin Index: {davies_bouldin_scores[optimal_k-2]:.3f}")

# applying k-means clustering
kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)   # random_state = 42 so results stay consistent
df['Cluster_Label'] = kmeans.fit_predict(X_scaled)

# Cluster Quality Metrics
silhouette_avg = silhouette_score(X_scaled, kmeans.labels_)
davies_bouldin = davies_bouldin_score(X_scaled, kmeans.labels_)

print("CLUSTER QUALITY VALIDATION")

print(f"Number of Clusters: {kmeans.n_clusters}")
print(f"Silhouette Score: {silhouette_avg:.3f} (Range: -1 to 1, Higher is Better)")
print(f"Davies-Bouldin Index: {davies_bouldin:.3f} (Lower is Better)")

if silhouette_avg > 0.5:
    print("GOOD: Clusters are well-separated and cohesive")
elif silhouette_avg > 0.25:
    print("FAIR: Clusters have some overlap")
else:
    print("POOR: Clusters are not well-defined")

# Silhouette Plot for Visual Validation
fig, ax = plt.subplots(figsize=(12, 6))

silhouette_vals = silhouette_samples(X_scaled, kmeans.labels_)
y_lower = 10

colors = plt.cm.Set3(np.linspace(0, 1, kmeans.n_clusters))

for i in range(kmeans.n_clusters):
    cluster_silhouette_vals = silhouette_vals[kmeans.labels_ == i]
    cluster_silhouette_vals.sort()

    size_cluster_i = cluster_silhouette_vals.shape[0]
    y_upper = y_lower + size_cluster_i

    ax.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_silhouette_vals,
                      facecolor=colors[i], edgecolor=colors[i], alpha=0.7)

    # Label the clusters
    ax.text(-0.05, y_lower + 0.5 * size_cluster_i, f'Cluster {i}', fontsize=10)

    y_lower = y_upper + 10

ax.axvline(x=silhouette_avg, color="red", linestyle="--", linewidth=2,
           label=f'Average Silhouette Score: {silhouette_avg:.2f}')
ax.set_xlabel("Silhouette Coefficient Values", fontsize=12)
ax.set_ylabel("Cluster Label", fontsize=12)
ax.set_title("Silhouette Plot for KMeans Clustering", fontsize=14, fontweight='bold')
ax.legend(fontsize=12)
ax.set_yticks([])
ax.grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.show()

# Print cluster distribution
print(f"\nCluster Distribution:")
cluster_counts = pd.Series(kmeans.labels_).value_counts().sort_index()
for cluster_id, count in cluster_counts.items():
    percentage = (count / len(kmeans.labels_)) * 100
    print(f"Cluster {cluster_id}: {count} samples ({percentage:.1f}%)")

# Identify which cluster is Good
# K-means doesn't know Good v/s Bad, it knows Group 0 and Group 1
# check which group has the higher average reputation
cluster_0_avg = df[df['Cluster_Label'] == 0]['Reputation'].mean()
cluster_1_avg = df[df['Cluster_Label'] == 1]['Reputation'].mean()

if cluster_1_avg > cluster_0_avg:
    df['Recruiter_Label'] = df['Cluster_Label'].map({1: 'Good', 0: 'Bad'})
else:
    df['Recruiter_Label'] = df['Cluster_Label'].map({0: 'Good', 1: 'Bad'})

print(df['Recruiter_Label'].value_counts())

# BAR CHART: Talent Distribution
plt.figure(figsize=(7, 5))
sns.countplot(x='Recruiter_Label', data=df, palette={'Good': 'green', 'Bad': 'blue'})
plt.title('Distribution of Candidate Quality (K-Means)')
plt.ylabel('Number of Candidates')
plt.xlabel('Category')
plt.savefig('../ML_Visualizations/stackoverflow/stackoverflow_distribution_counts.png')

# SCATTER PLOT: Reputation v/s Badges
# This shows how the Good group is physically separated in the data
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='Reputation', y='Silver_Badges', hue='Recruiter_Label',
                palette={'Good': 'green', 'Bad': 'blue'}, alpha=0.8)
plt.title('Talent Identification: Reputation v/s Silver Badges')
plt.savefig('../ML_Visualizations/stackoverflow/stackoverflow_feature_scatter.png')

# PCA PLOT: High-Dimensional Separation
# This shows how the AI sees the clusters using all 4 features at once
X_scaled = StandardScaler().fit_transform(df[all_features])

pca = PCA(n_components=2)
coords = pca.fit_transform(X_scaled)

plt.figure(figsize=(10, 6))
sns.scatterplot(x=coords[:, 0], y=coords[:, 1], hue=df['Recruiter_Label'],
                palette={'Good': 'green', 'Bad': 'blue'}, alpha=0.5)
plt.title('Cluster Separation in 2D (PCA Projection)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.savefig('../ML_Visualizations/stackoverflow/stackoverflow_pca_cluster_viz.png')

"""## Train - Test Split"""

# PREPARE DATA
X = df[all_features]
y = df['Cluster_Label']

# TRAIN/TEST SPLIT
# We split first so the test set remains pure real world data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle = True)

"""# SMOTE"""

# APPLY SMOTE (To TRAINING only)
sm = SMOTE(random_state=42)
X_train_res, y_train_res = sm.fit_resample(X_train, y_train)

# Visualization plot to show the effect of SMOTE on the training data distribution

before_counts = pd.Series(y_train).value_counts().sort_index()
after_counts = pd.Series(y_train_res).value_counts().sort_index()

# Set up the plot
fig, ax = plt.subplots(1, 2, figsize=(14, 6), sharey=False)

# Plot Before SMOTE
sns.barplot(x=['Bad (0)', 'Good (1)'], y=before_counts.values, ax=ax[0], palette='Blues')
ax[0].set_title('Training Set: Before SMOTE (Imbalanced)')
ax[0].set_ylabel('Number of Samples')

# Plot After SMOTE
sns.barplot(x=['Bad (0)', 'Good (1)'], y=after_counts.values, ax=ax[1], palette='crest')
ax[1].set_title('Training Set: After SMOTE (Balanced)')
ax[1].set_ylabel('Number of Samples')

plt.tight_layout()
plt.show()

# Using PCA to visualize the resampled data into 2D
pca = PCA(n_components=2)
X_pca_res = pca.fit_transform(X_train_res)

plt.figure(figsize=(10, 7))
sns.scatterplot(x=X_pca_res[:, 0], y=X_pca_res[:, 1], hue=y_train_res, palette={1: 'green', 0: 'blue'}, alpha=0.4)
plt.title('PCA of Training Data After SMOTE')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend(title='Category (0=Bad, 1=Good)')
plt.show()

"""# Supervised Learning

# Random Forest Classifier
"""

# Initialize the Random Forest
# n_estimators = 100 means we are using 100 individual decision trees
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Training the model using SMOTE balanced training data
rf_model.fit(X_train_res, y_train_res)

# Evaluate on Real World unbalanced test set
y_pred = rf_model.predict(X_test)
print("Model Performance:")
print(classification_report(y_test, y_pred))

# Generating the matrix
cm = confusion_matrix(y_test, y_pred)

# Visualizing it
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Bad Pick', 'Good Pick'],
            yticklabels=['Bad Pick', 'Good Pick'])
plt.title('Confusion Matrix: RANDOM FOREST')
plt.xlabel('Predicted Label')
plt.ylabel('Actual Label (from K-Means)')
plt.show()

# ROC Curve for Random Forest
fpr, tpr, _ = roc_curve(y_test, y_pred)
roc_auc = auc(fpr, tpr)

plt.title("ROC Curve for Random Forest")
plt.plot(fpr, tpr, label=f"AUC={roc_auc:.2f}")
plt.plot([0,1],[0,1],'--')
plt.legend()
plt.show()

"""## Ranking Candidates using Random Forest"""

# RANKING: Use the model to find the Top 5 Picks
# Get the probability of being Good (1 class)
df['Selection_Probability'] = rf_model.predict_proba(X)[:, 1]

# Sort by probability, then by Reputation
top_10_candidates_rfm = df.sort_values(by=['Selection_Probability'], ascending=False).head(10)

print("\n TOP 10 CANDIDATE PICKS")
print(top_10_candidates_rfm[['Display_Name', 'Reputation', 'Gold_Badges', 'Silver_Badges', 'Bronze_Badges', 'Selection_Probability']])

"""## DecisionTree"""

# Initialize the Decision Tree
# max_depth limits how deep the tree can grow to prevent overfitting
dt_model = DecisionTreeClassifier(max_depth=10, random_state=42)

# Training the model using SMOTE balanced training data
dt_model.fit(X_train_res, y_train_res)

# Evaluate on Real World unbalanced test set
y_pred_dt = dt_model.predict(X_test)
print("Decision Tree Model Performance")
print(classification_report(y_test, y_pred_dt))

# ROC Curve for Decision Tree
fpr_dt, tpr_dt, _ = roc_curve(y_test, y_pred_dt)
roc_auc_dt = auc(fpr_dt, tpr_dt)

plt.title("ROC Curve for Decision Tree")
plt.plot(fpr_dt, tpr_dt, label=f"AUC={roc_auc_dt:.2f}")
plt.plot([0,1],[0,1],'--')
plt.legend()
plt.show()

# RANKING: Use the model to find the Top 10 Picks
# Get the probability of being Good (1 class)
df['Selection_Probability_dt'] = dt_model.predict_proba(X)[:, 1]

# Sort by probability, then by Reputation
top_10_candidates_dt = df.sort_values(by=['Selection_Probability_dt'], ascending=False).head(10)

print("\n TOP 10 CANDIDATE PICKS")
print(top_10_candidates_dt[['Display_Name', 'Reputation', 'Gold_Badges', 'Silver_Badges', 'Bronze_Badges', 'Selection_Probability_dt']])

"""## XGBoost"""

# Initialize the XGBoost Classifier
xgb_model = XGBClassifier(n_estimators=100, eval_metric='logloss', random_state=42)

# Training the model using SMOTE balanced training data
xgb_model.fit(X_train_res, y_train_res)

# Evaluate on Real World unbalanced test set
y_pred_xgb = xgb_model.predict(X_test)
print("XGBoost Model Performance")
print(classification_report(y_test, y_pred_xgb))

# ROC Curve for XGBoost
fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_pred_xgb)
roc_auc_xgb = auc(fpr_xgb, tpr_xgb)

plt.title("ROC Curve for XGBoost")
plt.plot(fpr_xgb, tpr_xgb, label=f"AUC={roc_auc_xgb:.2f}")
plt.plot([0,1],[0,1],'--')
plt.legend()
plt.show()

# RANKING: Use the model to find the Top 10 Picks
# Get the probability of being Good (1 class)
df['Selection_Probability_xgb'] = xgb_model.predict_proba(X)[:, 1]

# Sort by probability, then by Reputation
top_10_candidates_xgb = df.sort_values(by=['Selection_Probability_xgb'], ascending=False).head(10)

print("\n TOP 10 CANDIDATE PICKS")
print(top_10_candidates_xgb[['Display_Name', 'Reputation', 'Gold_Badges', 'Silver_Badges', 'Bronze_Badges', 'Selection_Probability_xgb']])

"""## Compare All Model Performances"""

print("MODEL PERFORMANCE COMPARISON")

models = {
    'Random Forest': rf_model,
    'Decision Tree': dt_model,
    'XGBoost': xgb_model
}

# Store results
results = []
best_model_name = None
best_f1 = 0

for name, model in models.items():
    y_pred = model.predict(X_test)

    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    results.append({
        'Model': name,
        'Accuracy': acc,
        'Precision': prec,
        'Recall': rec,
        'F1-Score': f1
    })

    print(f"\n{name}:")
    print(f"  Accuracy:  {acc:.3f}")
    print(f"  Precision: {prec:.3f}")
    print(f"  Recall:    {rec:.3f}")
    print(f"  F1-Score:  {f1:.3f}")

    if f1 > best_f1:
        best_f1 = f1
        best_model_name = name

print(f"BEST MODEL: {best_model_name} (F1-Score: {best_f1:.3f})")

# Create comparison DataFrame
comparison_df = pd.DataFrame(results)
print("\nModel Comparison Table:")
print(comparison_df.to_string(index=False))

# Visualize comparison
fig, ax = plt.subplots(figsize=(12, 6))
x = np.arange(len(results))
width = 0.2

metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
colors = ["#011b2d", "#0785B2", "#01461e", "#0ab448"]

for i, metric in enumerate(metrics):
    values = [r[metric] for r in results]
    ax.bar(x + i*width, values, width, label=metric, color=colors[i])

ax.set_xlabel('Models', fontsize=12)
ax.set_ylabel('Score', fontsize=12)
ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')
ax.set_xticks(x + width * 1.5)
ax.set_xticklabels([r['Model'] for r in results])
ax.legend()
ax.grid(True, alpha=0.3, axis='y')
ax.set_ylim([0, 1.1])

plt.tight_layout()
plt.show()

# Use best model for final predictions
final_model = models[best_model_name]
print(f"\nUsing {best_model_name} for final predictions")

# Save model and scaler to pickle file
with open('stackoverflow_model.pkl', 'wb') as f:
    pickle.dump({'model': final_model, 'scaler': scaler}, f)

print("Model saved!")

"""## Importance Score"""

# Get importance scores
importances = rf_model.feature_importances_

plt.figure(figsize=(10, 5))
plt.barh(all_features, importances, color='darkblue')
plt.title('Which Features Define a Good Candidate?')
plt.xlabel('Importance Score (0.0 to 1.0)')
plt.show()

"""# Export to CSV"""

# RANKING: Use the BEST Model (selected from comparison) to find the Top 10 Picks
# Note: Make sure you've run SNIPPET 3 first to define 'final_model' and 'best_model_name'

# Recalculate predictions using the best model
df['Selection_Probability'] = final_model.predict_proba(X)[:, 1]

# Sort by probability and get top 10
top_10_candidates = df.sort_values(by=['Selection_Probability'], ascending=False).head(10)

print(f"TOP 10 CANDIDATE PICKS (Using {best_model_name})")
print(top_10_candidates[['Display_Name', 'Reputation', 'Gold_Badges', 'Silver_Badges', 'Bronze_Badges', 'Selection_Probability']])

# Save the top 10 candidates to a CSV file
top_10_candidates.to_csv('../Shortlisted_candidates/stackoverflow_shortlisted_candidates.csv', index=False)

print(f"\nTop 10 candidates saved to 'stackoverflow_shortlisted_candidates.csv'")